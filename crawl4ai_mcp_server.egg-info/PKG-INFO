Metadata-Version: 2.4
Name: crawl4ai-mcp-server
Version: 0.1.0
Summary: MCP server with crawl4ai integration for web content extraction
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: fastmcp>=2.0.0
Requires-Dist: crawl4ai[all]>=0.7.0
Requires-Dist: python-dotenv
Requires-Dist: pydantic>=2.0.0
Requires-Dist: chromadb>=0.4.0
Requires-Dist: sentence-transformers>=2.0.0
Requires-Dist: langchain-text-splitters>=0.2.0
Requires-Dist: fastapi>=0.116.1
Requires-Dist: uvicorn>=0.35.0
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Requires-Dist: pytest-asyncio; extra == "test"
Requires-Dist: pytest-cov; extra == "test"
Requires-Dist: httpx; extra == "test"
Requires-Dist: tomli; extra == "test"
Provides-Extra: dev
Requires-Dist: mypy; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: bandit; extra == "dev"
Requires-Dist: safety; extra == "dev"

# Crawl4AI MCP Server

MCP Server f√ºr Web-Content-Extraktion mit [Crawl4AI](https://github.com/unclecode/crawl4ai) und RAG Knowledge Base Funktionalit√§t.

## Installation

### Basis Installation
```bash
git clone <repository-url>
cd crawl4ai-mcp-server
uv install
playwright install
```

### RAG Knowledge Base (Optional)
F√ºr erweiterte Funktionalit√§t mit semantischer Suche und Wissensspeicherung:

```bash
# RAG Dependencies installieren
pip install chromadb sentence-transformers langchain-text-splitters numpy

# Oder mit uv
uv add chromadb sentence-transformers langchain-text-splitters numpy
```

**Hinweis:** Ohne RAG Dependencies sind nur die 3 Basis-Tools verf√ºgbar. Mit RAG Dependencies erweitert sich die Funktionalit√§t auf 7 Tools.

## MCP Inspector Setup

```bash
npm install -g @modelcontextprotocol/inspector
mcp-inspector --config mcp-inspector-config.json --server crawl4ai
```

√ñffnet Browser auf http://localhost:3000

## Claude Desktop Integration

Konfiguration in `~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "crawl4ai": {
      "command": "/absolute/path/to/crawl4ai-mcp-server/.venv/bin/python",
      "args": ["/absolute/path/to/crawl4ai-mcp-server/server.py"]
    }
  }
}
```

**Alternative mit uv (falls global installiert):**
```json
{
  "mcpServers": {
    "crawl4ai": {
      "command": "uv",
      "args": ["run", "--directory", "/absolute/path/to/crawl4ai-mcp-server", "python", "server.py"]
    }
  }
}
```

## Verf√ºgbare Tools

### üåê Basis Web-Crawling Tools (immer verf√ºgbar)

#### `web_content_extract`
Extrahiert Content von einzelnen Webseiten als Markdown.

**Parameter:**
- `url` (string, required): URL der zu crawlenden Webseite

**Beispiel:**
```json
{
  "name": "web_content_extract",
  "arguments": {
    "url": "https://example.com"
  }
}
```

#### `domain_deep_crawl_tool`
Tiefes Crawling einer ganzen Domain mit konfigurierbaren Strategien.

**Parameter:**
- `domain_url` (string, required): Basis-URL/Domain zum Crawlen
- `max_depth` (int, default: 1): Maximale Crawl-Tiefe (0-10)
- `crawl_strategy` (string, default: "bfs"): Crawling-Strategie (bfs, dfs, best_first)
- `max_pages` (int, default: 10): Maximale Anzahl Seiten (1-1000)
- `include_external` (bool, default: false): Externe Links einschlie√üen
- `url_patterns` (list, optional): URL-Patterns zum Einschlie√üen
- `exclude_patterns` (list, optional): URL-Patterns zum Ausschlie√üen
- `keywords` (list, optional): Keywords f√ºr BestFirst-Scoring

#### `domain_link_preview_tool`
Schnelle Link-Vorschau einer Domain ohne vollst√§ndiges Crawling.

**Parameter:**
- `domain_url` (string, required): Basis-URL/Domain zum Analysieren  
- `include_external` (bool, default: false): Externe Links einschlie√üen

### üß† RAG Knowledge Base Tools (erfordert optionale Dependencies)

**Hinweis:** Diese Tools sind nur verf√ºgbar wenn RAG Dependencies installiert sind (siehe Installation).

#### `store_crawl_results`
Speichert Crawl-Ergebnisse in der vektorbasierten Knowledge Base f√ºr sp√§tere semantische Suche.

**Parameter:**
- `crawl_result` (string, required): Crawl-Ergebnis (String oder JSON)
- `collection_name` (string, default: "default"): Name der Collection

**Beispiel:**
```json
{
  "name": "store_crawl_results",
  "arguments": {
    "crawl_result": "Content text or JSON from crawling",
    "collection_name": "my_docs"
  }
}
```

#### `search_knowledge_base`
Semantische Suche in der gespeicherten Knowledge Base mit Similarity-Scoring.

**Parameter:**
- `query` (string, required): Suchanfrage
- `collection_name` (string, default: "default"): Collection zum Durchsuchen
- `n_results` (int, default: 5): Maximale Anzahl Ergebnisse (1-20)
- `similarity_threshold` (float, optional): Minimaler Similarity Score (0.0-1.0)

**Beispiel:**
```json
{
  "name": "search_knowledge_base",
  "arguments": {
    "query": "machine learning algorithms",
    "collection_name": "tech_docs",
    "n_results": 3
  }
}
```

#### `list_collections`
Listet alle verf√ºgbaren Collections in der Knowledge Base mit Statistiken auf.

**Parameter:** Keine

**Beispiel:**
```json
{
  "name": "list_collections",
  "arguments": {}
}
```

#### `delete_collection`
L√∂scht eine Collection permanent aus der Knowledge Base.

**Parameter:**
- `collection_name` (string, required): Name der zu l√∂schenden Collection

**Beispiel:**
```json
{
  "name": "delete_collection",
  "arguments": {
    "collection_name": "old_docs"
  }
}
```

## RAG Knowledge Base - Quick Start

### Grundlegende Konfiguration

Erstelle optional eine `.env` Datei f√ºr Konfiguration:
```env
# RAG Knowledge Base Konfiguration
RAG_DB_PATH=./my_knowledge_base    # Pfad zur ChromaDB Datenbank
RAG_MODEL_NAME=all-MiniLM-L6-v2    # Embedding Model
RAG_CHUNK_SIZE=1000                # Text Chunk Gr√∂√üe
RAG_CHUNK_OVERLAP=200              # Overlap zwischen Chunks
RAG_DEVICE=cpu                     # cpu oder cuda
```

### Typische Workflows

#### 1. Content Crawlen und Speichern
```json
// 1. Webseite crawlen
{
  "name": "web_content_extract",
  "arguments": {
    "url": "https://docs.python.org/3/tutorial/"
  }
}

// 2. Ergebnis in Knowledge Base speichern
{
  "name": "store_crawl_results", 
  "arguments": {
    "crawl_result": "[crawl output from step 1]",
    "collection_name": "python_docs"
  }
}
```

#### 2. Domain Crawlen und Batch-Speichern
```json
// 1. Ganze Domain crawlen
{
  "name": "domain_deep_crawl_tool",
  "arguments": {
    "domain_url": "https://fastapi.tiangolo.com",
    "max_depth": 2,
    "max_pages": 20
  }
}

// 2. Alle Ergebnisse speichern
{
  "name": "store_crawl_results",
  "arguments": {
    "crawl_result": "[domain crawl JSON output]",
    "collection_name": "fastapi_docs"
  }
}
```

#### 3. Semantische Suche
```json
// Nach relevantem Content suchen
{
  "name": "search_knowledge_base",
  "arguments": {
    "query": "async database connections",
    "collection_name": "fastapi_docs",
    "n_results": 5
  }
}
```

#### 4. Collection Management
```json
// Alle Collections anzeigen
{
  "name": "list_collections",
  "arguments": {}
}

// Collection l√∂schen
{
  "name": "delete_collection",
  "arguments": {
    "collection_name": "old_collection"
  }
}
```

### Integration mit Claude/ChatGPT

**Beispiel-Prompt:**
```
Bitte crawle die FastAPI Dokumentation und speichere sie in einer Knowledge Base:

1. Nutze domain_deep_crawl_tool f√ºr https://fastapi.tiangolo.com 
2. Speichere die Ergebnisse mit store_crawl_results in Collection "fastapi"
3. Suche dann nach "authentication" in der Collection
4. Fasse die gefundenen Informationen zusammen
```

## Tests

```bash
# Alle Tests
pytest

# Nur schnelle Tests (empfohlen f√ºr Entwicklung)
pytest -m "not slow"

# Mit Coverage
pytest --cov=. --cov-report=html

# RAG Tests (erfordern Dependencies)
pytest tests/test_rag_integration.py tests/test_knowledge_base.py
```

## Entwicklung

Server direkt starten:
```bash
uv run python server.py
```

## Troubleshooting

### Allgemeine Probleme
1. **Module not found**: Stelle sicher, dass alle Dependencies installiert sind: `uv install`
2. **Playwright fehlt**: Browser installieren: `playwright install`
3. **MCP Inspector verbindet nicht**: Pr√ºfe ob Server l√§uft und Config korrekt ist

### RAG-spezifische Probleme
4. **"RAG tools not available"**: RAG Dependencies installieren (siehe Installation)
5. **ChromaDB Fehler**: Pr√ºfe `RAG_DB_PATH` Berechtigung und Speicherplatz
6. **Embedding Model l√§dt nicht**: Internetverbindung pr√ºfen (Model wird bei erstem Start heruntergeladen)
7. **Speicher-Probleme bei gro√üen Dokumenten**: `RAG_CHUNK_SIZE` reduzieren oder `RAG_DEVICE=cpu` setzen
8. **Suche liefert keine Ergebnisse**: `similarity_threshold` reduzieren oder Collection mit `list_collections` pr√ºfen

### RAG Tools Test
```bash
# Pr√ºfen ob RAG Dependencies verf√ºgbar sind
python3 -c "from tools.knowledge_base.dependencies import is_rag_available; print('RAG available:', is_rag_available())"

# RAG funktionalit√§t testen
pytest tests/test_knowledge_base.py -v
```

## Links

- [Crawl4AI Documentation](https://github.com/unclecode/crawl4ai)
- [Model Context Protocol](https://spec.modelcontextprotocol.io/)
- [Claude Desktop Integration](https://docs.anthropic.com/en/docs/build-with-claude/mcp)
