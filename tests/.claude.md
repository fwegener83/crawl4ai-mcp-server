# Test Configuration Notes for Claude Code

## Important CI Behavior

⚠️ **CRITICAL**: When `CI=true` environment variable is set, many E2E tests are **SKIPPED** in the CI pipeline.

This is why running:
```bash
CI=true uv run pytest tests/e2e_api/test_05_vector_sync_and_search.py::test_sync_nonexistent_collection -v -s
```

Results in:
```
tests/e2e_api/test_05_vector_sync_and_search.py::test_sync_nonexistent_collection SKIPPED
```

## Correct Test Commands

### For Development/Local Testing
```bash
# Run without CI flag - tests will execute normally
uv run pytest tests/e2e_api/test_05_vector_sync_and_search.py::test_sync_nonexistent_collection -v -s

# Run all vector-related tests
uv run pytest tests/ -v -k "vector" --tb=short

# Run integration tests  
uv run pytest tests/test_vector_sync_integration.py tests/test_api_integration.py -v
```

### For CI Pipeline
```bash
# CI environment automatically sets CI=true, which skips certain tests
# This is by design to avoid flaky tests in CI
```

## Test Categories

### Always Available (No CI Skip)
- Unit tests: `tests/test_*.py` 
- Integration tests: Most integration tests run in CI
- Schema validation tests
- Service layer tests

### CI-Conditional (May Skip with CI=true)
- E2E API tests: `tests/e2e_api/`
- Vector sync tests requiring RAG dependencies
- HTTP endpoint tests requiring server startup
- Tests marked with `@pytest.mark.skipif(os.getenv("CI"))`

## Testing Strategy for Development

1. **Local Development**: Run tests without `CI=true` to get full test execution
2. **Parameter Migration Verification**: Use integration tests instead of E2E when CI skips
3. **HTTP Status Code Testing**: Use unit tests with mocked HTTP clients rather than full E2E

## RAG Dependencies Note

Some tests also check for RAG availability:
```python
from tools.knowledge_base.dependencies import is_rag_available
```

If RAG dependencies (ChromaDB, sentence-transformers) are not installed, vector-related tests may also be skipped.

## Current Known Skip Patterns

Based on test runs, these are likely to be skipped in CI:
- `test_sync_nonexistent_collection`
- `test_get_status_nonexistent_collection` 
- `test_search_with_invalid_parameters`
- Other E2E API tests in `tests/e2e_api/`

## Recommended Approach

For verifying HTTP status code implementations:
1. Use integration tests: `tests/test_api_integration.py`
2. Use vector sync integration: `tests/test_vector_sync_integration.py`
3. Check service layer tests: `tests/test_*_service.py`
4. Only use E2E tests locally (without CI=true)

## AsyncMock and Mocking Best Practices

### Common AsyncMock Issue: "object Mock can't be used in 'await' expression"

This error occurs when trying to await regular `Mock()` objects in async tests. 

#### Root Cause Analysis (RAG API Integration Tests Example)

**Problem**: The RAG API integration tests were failing with:
```
ERROR    unified_server:unified_server.py:1264 HTTP rag_query error: object Mock can't be used in 'await' expression
```

**Root Cause**: Two-layer mocking problem:
1. **Incorrect Service Mocking**: Services with async methods were mocked as regular `Mock()` instead of `AsyncMock()`
2. **Missing Use-Case Patching**: HTTP tests were trying to mock services but not the use-case function itself

#### Solution Pattern

**❌ Broken Pattern (Original)**:
```python
# This fails because llm_service.generate_response() is awaited but Mock isn't awaitable
mock_llm_service = Mock()
mock_llm_service.generate_response = AsyncMock(return_value={...})
server.container.llm_service = Mock(return_value=mock_llm_service)  # Provider pattern broken
```

**✅ Working Pattern (Fixed)**:
```python
# Approach 1: Patch the use-case function directly (recommended for integration tests)
with patch('unified_server.rag_query_use_case', mock_rag_use_case):
    server = UnifiedServer()
    # Services still need to be mocked for other operations
    mock_llm_service = AsyncMock()
    server.container.llm_service = lambda: mock_llm_service

# Approach 2: Use AsyncMock for all async services (for unit tests)
mock_llm_service = AsyncMock()  # Entire service is AsyncMock
mock_llm_service.generate_response = AsyncMock(return_value={...})
server.container.llm_service = lambda: mock_llm_service  # Provider as lambda
```

#### Key Learnings

1. **Use AsyncMock for Async Services**: Any service with async methods should be mocked as `AsyncMock()`
2. **Provider Pattern**: Dependency injection containers often use provider functions. Mock as `lambda: mock_service` not `Mock(return_value=...)`
3. **Integration vs Unit Test Strategy**:
   - **Integration Tests**: Patch the top-level use-case function
   - **Unit Tests**: Mock individual service dependencies
4. **Consistent Patching**: MCP and HTTP tests must use the same patching strategy for consistency

#### Testing Patterns by Test Type

**Integration Tests (API Layer)**:
```python
with patch('unified_server.rag_query_use_case', mock_rag_use_case):
    # Test the API endpoint directly, mock the business logic
```

**Unit Tests (Service Layer)**:
```python
mock_vector_service = AsyncMock()
mock_collection_service = AsyncMock() 
mock_llm_service = AsyncMock()
# Test business logic with mocked dependencies
```

**E2E Tests**:
```python
# Use real services, mock only external dependencies (OpenAI API, ChromaDB)
```

#### Common AsyncMock Gotchas

1. **Mixing Mock and AsyncMock**: Don't mix regular Mock with AsyncMock for the same service
2. **Provider Functions**: Container providers should be mocked as lambdas, not Mock(return_value=...)
3. **Patch Scope**: Ensure patches target the correct import location
4. **Test Isolation**: Each test should clean up its mocks properly (pytest fixtures handle this automatically)