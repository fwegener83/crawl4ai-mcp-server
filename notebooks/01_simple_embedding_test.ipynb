{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Debugging - Step 1: Embedding Tests (Simplified)\n",
    "\n",
    "**Problem**: RAG findet kaum relevante Chunks bei deutschen/englischen Queries\n",
    "\n",
    "**Hypothese**: Das Embedding-Modell ist schlecht f√ºr multilinguale Inhalte\n",
    "\n",
    "**Test**: Direkt mit sentence-transformers testen (umgeht dependency issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Direkt mit sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Einfacher direkter Ansatz\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# Einfacher direkter Ansatz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ sentence-transformers loaded\")\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Simple cosine similarity function\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(\"‚úÖ Helper functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data - Realistische RAG-Beispiele\n",
    "\n",
    "Das sind typische Inhalte aus technischer Dokumentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deutsche Tech-Texte (typische Chunk-Inhalte)\n",
    "german_docs = [\n",
    "    \"Die Vektorsuche in ChromaDB verwendet Cosinus-√Ñhnlichkeit f√ºr semantische Suchen.\",\n",
    "    \"Chunking-Strategien sollten bei technischen Dokumentationen header-bewusst sein.\",\n",
    "    \"Der Similarity-Threshold von 0.7 ist oft zu restriktiv f√ºr multilinguale Inhalte.\"\n",
    "]\n",
    "\n",
    "# Englische Tech-Texte (semantisch verwandt)\n",
    "english_docs = [\n",
    "    \"Vector search in ChromaDB uses cosine similarity for semantic searches.\",\n",
    "    \"Chunking strategies should be header-aware for technical documentation.\",\n",
    "    \"The similarity threshold of 0.7 is often too restrictive for multilingual content.\"\n",
    "]\n",
    "\n",
    "# Typische User-Queries\n",
    "queries = {\n",
    "    \"german\": \"Wie funktioniert Vektorsuche?\",\n",
    "    \"english\": \"How does vector search work?\",\n",
    "    \"mixed\": \"ChromaDB similarity threshold\"\n",
    "}\n",
    "\n",
    "print(f\"üìù Test Setup:\")\n",
    "print(f\"   - {len(german_docs)} deutsche Dokumente\")\n",
    "print(f\"   - {len(english_docs)} englische Dokumente\") \n",
    "print(f\"   - {len(queries)} Test-Queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Aktuelles Modell (all-MiniLM-L6-v2)\n",
    "\n",
    "Das ist das Modell, das momentan in deinem RAG-System verwendet wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das aktuelle RAG-Modell laden\n",
    "print(\"ü§ñ Loading current model: all-MiniLM-L6-v2\")\n",
    "current_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"üìê Max sequence length: {current_model.max_seq_length}\")\n",
    "print(f\"üìä Embedding dimension: {current_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Encode all documents\n",
    "print(\"\\nüîÑ Encoding documents...\")\n",
    "de_embeddings = current_model.encode(german_docs)\n",
    "en_embeddings = current_model.encode(english_docs)\n",
    "\n",
    "print(f\"‚úÖ Encoded {len(de_embeddings + en_embeddings)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Cross-Language Similarity Matrix\n",
    "\n",
    "**Das ist der kritische Test**: Erkennt das Modell, dass deutsche und englische Texte mit gleicher Bedeutung √§hnlich sind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Cross-Language Similarity Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (de_text, de_emb) in enumerate(zip(german_docs, de_embeddings)):\n",
    "    print(f\"\\nüá©üá™ German Doc {i+1}: {de_text[:50]}...\")\n",
    "    \n",
    "    for j, (en_text, en_emb) in enumerate(zip(english_docs, en_embeddings)):\n",
    "        similarity = cosine_similarity(de_emb, en_emb)\n",
    "        \n",
    "        # Erwartete Paare (gleiche Bedeutung) highlighten\n",
    "        marker = \"üéØ\" if i == j else \"  \"\n",
    "        \n",
    "        print(f\"   {marker} vs EN Doc {j+1}: {similarity:.3f}\")\n",
    "        if i == j:\n",
    "            print(f\"       üá¨üáß {en_text[:50]}...\")\n",
    "            # Bewertung der Similarity\n",
    "            if similarity > 0.7:\n",
    "                print(f\"       ‚úÖ Sehr gut erkannt!\")\n",
    "            elif similarity > 0.5:\n",
    "                print(f\"       ‚ö†Ô∏è  Okay erkannt\")\n",
    "            else:\n",
    "                print(f\"       ‚ùå Schlecht erkannt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Query-Retrieval Simulation\n",
    "\n",
    "Simulieren wir eine echte RAG-Suche: Welche Dokumente w√ºrden f√ºr jede Query gefunden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombiniere alle Dokumente f√ºr die Suche\n",
    "all_docs = german_docs + english_docs\n",
    "all_embeddings = np.vstack([de_embeddings, en_embeddings])\n",
    "\n",
    "def test_query(query_text, query_name):\n",
    "    print(f\"\\nüîç Query Test: {query_name}\")\n",
    "    print(f\"Query: '{query_text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Query embedding\n",
    "    query_emb = current_model.encode([query_text])[0]\n",
    "    \n",
    "    # Similarity zu allen Dokumenten\n",
    "    similarities = []\n",
    "    for i, (doc, doc_emb) in enumerate(zip(all_docs, all_embeddings)):\n",
    "        sim = cosine_similarity(query_emb, doc_emb)\n",
    "        lang = \"üá©üá™\" if i < len(german_docs) else \"üá¨üáß\"\n",
    "        similarities.append((sim, doc, lang, i+1))\n",
    "    \n",
    "    # Sortiere nach Relevanz\n",
    "    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Zeige Results\n",
    "    print(\"Top Results:\")\n",
    "    for rank, (score, doc, lang, doc_id) in enumerate(similarities[:3], 1):\n",
    "        print(f\"  {rank}. [{score:.3f}] {lang} Doc{doc_id}: {doc[:60]}...\")\n",
    "    \n",
    "    # RAG-Threshold Analysis\n",
    "    above_07 = sum(1 for score, _, _, _ in similarities if score >= 0.7)\n",
    "    above_05 = sum(1 for score, _, _, _ in similarities if score >= 0.5)\n",
    "    above_03 = sum(1 for score, _, _, _ in similarities if score >= 0.3)\n",
    "    \n",
    "    print(f\"\\nüìä Threshold Analysis:\")\n",
    "    print(f\"   - Above 0.7: {above_07}/{len(similarities)} docs (current RAG threshold)\")\n",
    "    print(f\"   - Above 0.5: {above_05}/{len(similarities)} docs\")\n",
    "    print(f\"   - Above 0.3: {above_03}/{len(similarities)} docs\")\n",
    "    \n",
    "    if above_07 == 0:\n",
    "        print(f\"   ‚ùå Bei 0.7 Threshold: KEINE Ergebnisse!\")\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# Teste alle Queries\n",
    "results = {}\n",
    "for query_name, query_text in queries.items():\n",
    "    results[query_name] = test_query(query_text, query_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse: Was sehen wir?\n",
    "\n",
    "**Jetzt k√∂nnen wir konkret bewerten:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ ZUSAMMENFASSUNG DER ERKENNTNISSE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cross-Language Performance\n",
    "cross_lang_scores = []\n",
    "for i in range(len(german_docs)):\n",
    "    score = cosine_similarity(de_embeddings[i], en_embeddings[i])\n",
    "    cross_lang_scores.append(score)\n",
    "\n",
    "avg_cross_lang = np.mean(cross_lang_scores)\n",
    "print(f\"üìä Cross-Language Performance:\")\n",
    "print(f\"   - Durchschnitt DE<->EN: {avg_cross_lang:.3f}\")\n",
    "print(f\"   - Range: {min(cross_lang_scores):.3f} - {max(cross_lang_scores):.3f}\")\n",
    "\n",
    "if avg_cross_lang < 0.5:\n",
    "    print(f\"   ‚ùå PROBLEM: Sehr schlechte Cross-Language Performance!\")\n",
    "elif avg_cross_lang < 0.7:\n",
    "    print(f\"   ‚ö†Ô∏è  PROBLEM: M√§√üige Cross-Language Performance\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Gute Cross-Language Performance\")\n",
    "\n",
    "# Query Performance\n",
    "print(f\"\\nüîç Query Performance:\")\n",
    "for query_name, query_results in results.items():\n",
    "    best_score = query_results[0][0]\n",
    "    above_threshold = sum(1 for score, _, _, _ in query_results if score >= 0.7)\n",
    "    \n",
    "    print(f\"   - {query_name}: Best={best_score:.3f}, Above0.7={above_threshold}\")\n",
    "    \n",
    "print(f\"\\nüí° EMPFEHLUNGEN:\")\n",
    "if avg_cross_lang < 0.5:\n",
    "    print(f\"   1. üö® DRINGEND: Multilingual Model verwenden!\")\n",
    "    print(f\"   2. üìâ Similarity Threshold auf 0.3 reduzieren\")\n",
    "    print(f\"   3. üß™ Test: paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "else:\n",
    "    print(f\"   1. üìâ Similarity Threshold anpassen\")\n",
    "    print(f\"   2. üîß Query-Enhancement implementieren\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Based on the results above, our next notebook will test:**\n",
    "\n",
    "1. **Multilingual Models**: `paraphrase-multilingual-MiniLM-L12-v2`\n",
    "2. **Optimized Thresholds**: Find the sweet spot for your use case\n",
    "3. **Query Enhancement**: Add synonyms and translations\n",
    "\n",
    "---\n",
    "\n",
    "üéØ **This notebook shows the EXACT problem** - no complex RAG pipeline needed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
