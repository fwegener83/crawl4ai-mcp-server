{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Debugging - Step 1: Embedding Tests (Simplified)\n",
    "\n",
    "**Problem**: RAG findet kaum relevante Chunks bei deutschen/englischen Queries\n",
    "\n",
    "**Hypothese**: Das Embedding-Modell ist schlecht fÃ¼r multilinguale Inhalte\n",
    "\n",
    "**Test**: Direkt mit sentence-transformers testen (umgeht dependency issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Direkt mit sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florianwegener/Projects/crawl4ai-mcp-server/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… sentence-transformers loaded\n",
      "âœ… Helper functions ready\n"
     ]
    }
   ],
   "source": [
    "# Einfacher direkter Ansatz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"âœ… sentence-transformers loaded\")\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Simple cosine similarity function\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(\"âœ… Helper functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data - Realistische RAG-Beispiele\n",
    "\n",
    "Das sind typische Inhalte aus technischer Dokumentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Test Setup:\n",
      "   - 3 deutsche Dokumente\n",
      "   - 3 englische Dokumente\n",
      "   - 3 Test-Queries\n"
     ]
    }
   ],
   "source": [
    "# Deutsche Tech-Texte (typische Chunk-Inhalte)\n",
    "german_docs = [\n",
    "    \"Die Vektorsuche in ChromaDB verwendet Cosinus-Ã„hnlichkeit fÃ¼r semantische Suchen.\",\n",
    "    \"Chunking-Strategien sollten bei technischen Dokumentationen header-bewusst sein.\",\n",
    "    \"Der Similarity-Threshold von 0.7 ist oft zu restriktiv fÃ¼r multilinguale Inhalte.\"\n",
    "]\n",
    "\n",
    "# Englische Tech-Texte (semantisch verwandt)\n",
    "english_docs = [\n",
    "    \"Vector search in ChromaDB uses cosine similarity for semantic searches.\",\n",
    "    \"Chunking strategies should be header-aware for technical documentation.\",\n",
    "    \"The similarity threshold of 0.7 is often too restrictive for multilingual content.\"\n",
    "]\n",
    "\n",
    "# Typische User-Queries\n",
    "queries = {\n",
    "    \"german\": \"Wie funktioniert Vektorsuche?\",\n",
    "    \"english\": \"How does vector search work?\",\n",
    "    \"mixed\": \"ChromaDB similarity threshold\"\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“ Test Setup:\")\n",
    "print(f\"   - {len(german_docs)} deutsche Dokumente\")\n",
    "print(f\"   - {len(english_docs)} englische Dokumente\") \n",
    "print(f\"   - {len(queries)} Test-Queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Aktuelles Modell (all-MiniLM-L6-v2)\n",
    "\n",
    "Das ist das Modell, das momentan in deinem RAG-System verwendet wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Loading current model: all-MiniLM-L6-v2\n",
      "ğŸ“ Max sequence length: 256\n",
      "ğŸ“Š Embedding dimension: 384\n",
      "\n",
      "ğŸ”„ Encoding documents...\n",
      "âœ… Encoded 3 documents\n"
     ]
    }
   ],
   "source": [
    "# Das aktuelle RAG-Modell laden\n",
    "print(\"ğŸ¤– Loading current model: all-MiniLM-L6-v2\")\n",
    "current_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"ğŸ“ Max sequence length: {current_model.max_seq_length}\")\n",
    "print(f\"ğŸ“Š Embedding dimension: {current_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Encode all documents\n",
    "print(\"\\nğŸ”„ Encoding documents...\")\n",
    "de_embeddings = current_model.encode(german_docs)\n",
    "en_embeddings = current_model.encode(english_docs)\n",
    "\n",
    "print(f\"âœ… Encoded {len(de_embeddings + en_embeddings)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Cross-Language Similarity Matrix\n",
    "\n",
    "**Das ist der kritische Test**: Erkennt das Modell, dass deutsche und englische Texte mit gleicher Bedeutung Ã¤hnlich sind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cross-Language Similarity Analysis\n",
      "============================================================\n",
      "\n",
      "ğŸ‡©ğŸ‡ª German Doc 1: Die Vektorsuche in ChromaDB verwendet Cosinus-Ã„hnl...\n",
      "   ğŸ¯ vs EN Doc 1: 0.410\n",
      "       ğŸ‡¬ğŸ‡§ Vector search in ChromaDB uses cosine similarity f...\n",
      "       âŒ Schlecht erkannt!\n",
      "      vs EN Doc 2: -0.002\n",
      "      vs EN Doc 3: 0.094\n",
      "\n",
      "ğŸ‡©ğŸ‡ª German Doc 2: Chunking-Strategien sollten bei technischen Dokume...\n",
      "      vs EN Doc 1: 0.202\n",
      "   ğŸ¯ vs EN Doc 2: 0.594\n",
      "       ğŸ‡¬ğŸ‡§ Chunking strategies should be header-aware for tec...\n",
      "       âš ï¸  Okay erkannt\n",
      "      vs EN Doc 3: 0.244\n",
      "\n",
      "ğŸ‡©ğŸ‡ª German Doc 3: Der Similarity-Threshold von 0.7 ist oft zu restri...\n",
      "      vs EN Doc 1: 0.318\n",
      "      vs EN Doc 2: 0.144\n",
      "   ğŸ¯ vs EN Doc 3: 0.683\n",
      "       ğŸ‡¬ğŸ‡§ The similarity threshold of 0.7 is often too restr...\n",
      "       âš ï¸  Okay erkannt\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Cross-Language Similarity Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (de_text, de_emb) in enumerate(zip(german_docs, de_embeddings)):\n",
    "    print(f\"\\nğŸ‡©ğŸ‡ª German Doc {i+1}: {de_text[:50]}...\")\n",
    "    \n",
    "    for j, (en_text, en_emb) in enumerate(zip(english_docs, en_embeddings)):\n",
    "        similarity = cosine_similarity(de_emb, en_emb)\n",
    "        \n",
    "        # Erwartete Paare (gleiche Bedeutung) highlighten\n",
    "        marker = \"ğŸ¯\" if i == j else \"  \"\n",
    "        \n",
    "        print(f\"   {marker} vs EN Doc {j+1}: {similarity:.3f}\")\n",
    "        if i == j:\n",
    "            print(f\"       ğŸ‡¬ğŸ‡§ {en_text[:50]}...\")\n",
    "            # Bewertung der Similarity\n",
    "            if similarity > 0.7:\n",
    "                print(f\"       âœ… Sehr gut erkannt!\")\n",
    "            elif similarity > 0.5:\n",
    "                print(f\"       âš ï¸  Okay erkannt\")\n",
    "            else:\n",
    "                print(f\"       âŒ Schlecht erkannt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Query-Retrieval Simulation\n",
    "\n",
    "Simulieren wir eine echte RAG-Suche: Welche Dokumente wÃ¼rden fÃ¼r jede Query gefunden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Query Test: german\n",
      "Query: 'Wie funktioniert Vektorsuche?'\n",
      "--------------------------------------------------\n",
      "Top Results:\n",
      "  1. [0.502] ğŸ‡©ğŸ‡ª Doc1: Die Vektorsuche in ChromaDB verwendet Cosinus-Ã„hnlichkeit fÃ¼...\n",
      "  2. [0.280] ğŸ‡©ğŸ‡ª Doc3: Der Similarity-Threshold von 0.7 ist oft zu restriktiv fÃ¼r m...\n",
      "  3. [0.264] ğŸ‡©ğŸ‡ª Doc2: Chunking-Strategien sollten bei technischen Dokumentationen ...\n",
      "\n",
      "ğŸ“Š Threshold Analysis:\n",
      "   - Above 0.7: 0/6 docs (current RAG threshold)\n",
      "   - Above 0.5: 1/6 docs\n",
      "   - Above 0.3: 1/6 docs\n",
      "   âŒ Bei 0.7 Threshold: KEINE Ergebnisse!\n",
      "\n",
      "ğŸ” Query Test: english\n",
      "Query: 'How does vector search work?'\n",
      "--------------------------------------------------\n",
      "Top Results:\n",
      "  1. [0.577] ğŸ‡¬ğŸ‡§ Doc4: Vector search in ChromaDB uses cosine similarity for semanti...\n",
      "  2. [0.167] ğŸ‡©ğŸ‡ª Doc2: Chunking-Strategien sollten bei technischen Dokumentationen ...\n",
      "  3. [0.119] ğŸ‡©ğŸ‡ª Doc3: Der Similarity-Threshold von 0.7 ist oft zu restriktiv fÃ¼r m...\n",
      "\n",
      "ğŸ“Š Threshold Analysis:\n",
      "   - Above 0.7: 0/6 docs (current RAG threshold)\n",
      "   - Above 0.5: 1/6 docs\n",
      "   - Above 0.3: 1/6 docs\n",
      "   âŒ Bei 0.7 Threshold: KEINE Ergebnisse!\n",
      "\n",
      "ğŸ” Query Test: mixed\n",
      "Query: 'ChromaDB similarity threshold'\n",
      "--------------------------------------------------\n",
      "Top Results:\n",
      "  1. [0.611] ğŸ‡¬ğŸ‡§ Doc4: Vector search in ChromaDB uses cosine similarity for semanti...\n",
      "  2. [0.424] ğŸ‡©ğŸ‡ª Doc1: Die Vektorsuche in ChromaDB verwendet Cosinus-Ã„hnlichkeit fÃ¼...\n",
      "  3. [0.394] ğŸ‡©ğŸ‡ª Doc3: Der Similarity-Threshold von 0.7 ist oft zu restriktiv fÃ¼r m...\n",
      "\n",
      "ğŸ“Š Threshold Analysis:\n",
      "   - Above 0.7: 0/6 docs (current RAG threshold)\n",
      "   - Above 0.5: 1/6 docs\n",
      "   - Above 0.3: 4/6 docs\n",
      "   âŒ Bei 0.7 Threshold: KEINE Ergebnisse!\n"
     ]
    }
   ],
   "source": [
    "# Kombiniere alle Dokumente fÃ¼r die Suche\n",
    "all_docs = german_docs + english_docs\n",
    "all_embeddings = np.vstack([de_embeddings, en_embeddings])\n",
    "\n",
    "def test_query(query_text, query_name):\n",
    "    print(f\"\\nğŸ” Query Test: {query_name}\")\n",
    "    print(f\"Query: '{query_text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Query embedding\n",
    "    query_emb = current_model.encode([query_text])[0]\n",
    "    \n",
    "    # Similarity zu allen Dokumenten\n",
    "    similarities = []\n",
    "    for i, (doc, doc_emb) in enumerate(zip(all_docs, all_embeddings)):\n",
    "        sim = cosine_similarity(query_emb, doc_emb)\n",
    "        lang = \"ğŸ‡©ğŸ‡ª\" if i < len(german_docs) else \"ğŸ‡¬ğŸ‡§\"\n",
    "        similarities.append((sim, doc, lang, i+1))\n",
    "    \n",
    "    # Sortiere nach Relevanz\n",
    "    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Zeige Results\n",
    "    print(\"Top Results:\")\n",
    "    for rank, (score, doc, lang, doc_id) in enumerate(similarities[:3], 1):\n",
    "        print(f\"  {rank}. [{score:.3f}] {lang} Doc{doc_id}: {doc[:60]}...\")\n",
    "    \n",
    "    # RAG-Threshold Analysis\n",
    "    above_07 = sum(1 for score, _, _, _ in similarities if score >= 0.7)\n",
    "    above_05 = sum(1 for score, _, _, _ in similarities if score >= 0.5)\n",
    "    above_03 = sum(1 for score, _, _, _ in similarities if score >= 0.3)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Threshold Analysis:\")\n",
    "    print(f\"   - Above 0.7: {above_07}/{len(similarities)} docs (current RAG threshold)\")\n",
    "    print(f\"   - Above 0.5: {above_05}/{len(similarities)} docs\")\n",
    "    print(f\"   - Above 0.3: {above_03}/{len(similarities)} docs\")\n",
    "    \n",
    "    if above_07 == 0:\n",
    "        print(f\"   âŒ Bei 0.7 Threshold: KEINE Ergebnisse!\")\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# Teste alle Queries\n",
    "results = {}\n",
    "for query_name, query_text in queries.items():\n",
    "    results[query_name] = test_query(query_text, query_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse: Was sehen wir?\n",
    "\n",
    "**Jetzt kÃ¶nnen wir konkret bewerten:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ZUSAMMENFASSUNG DER ERKENNTNISSE\n",
      "==================================================\n",
      "ğŸ“Š Cross-Language Performance:\n",
      "   - Durchschnitt DE<->EN: 0.562\n",
      "   - Range: 0.410 - 0.683\n",
      "   âš ï¸  PROBLEM: MÃ¤ÃŸige Cross-Language Performance\n",
      "\n",
      "ğŸ” Query Performance:\n",
      "   - german: Best=0.502, Above0.7=0\n",
      "   - english: Best=0.577, Above0.7=0\n",
      "   - mixed: Best=0.611, Above0.7=0\n",
      "\n",
      "ğŸ’¡ EMPFEHLUNGEN:\n",
      "   1. ğŸ“‰ Similarity Threshold anpassen\n",
      "   2. ğŸ”§ Query-Enhancement implementieren\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¯ ZUSAMMENFASSUNG DER ERKENNTNISSE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cross-Language Performance\n",
    "cross_lang_scores = []\n",
    "for i in range(len(german_docs)):\n",
    "    score = cosine_similarity(de_embeddings[i], en_embeddings[i])\n",
    "    cross_lang_scores.append(score)\n",
    "\n",
    "avg_cross_lang = np.mean(cross_lang_scores)\n",
    "print(f\"ğŸ“Š Cross-Language Performance:\")\n",
    "print(f\"   - Durchschnitt DE<->EN: {avg_cross_lang:.3f}\")\n",
    "print(f\"   - Range: {min(cross_lang_scores):.3f} - {max(cross_lang_scores):.3f}\")\n",
    "\n",
    "if avg_cross_lang < 0.5:\n",
    "    print(f\"   âŒ PROBLEM: Sehr schlechte Cross-Language Performance!\")\n",
    "elif avg_cross_lang < 0.7:\n",
    "    print(f\"   âš ï¸  PROBLEM: MÃ¤ÃŸige Cross-Language Performance\")\n",
    "else:\n",
    "    print(f\"   âœ… Gute Cross-Language Performance\")\n",
    "\n",
    "# Query Performance\n",
    "print(f\"\\nğŸ” Query Performance:\")\n",
    "for query_name, query_results in results.items():\n",
    "    best_score = query_results[0][0]\n",
    "    above_threshold = sum(1 for score, _, _, _ in query_results if score >= 0.7)\n",
    "    \n",
    "    print(f\"   - {query_name}: Best={best_score:.3f}, Above0.7={above_threshold}\")\n",
    "    \n",
    "print(f\"\\nğŸ’¡ EMPFEHLUNGEN:\")\n",
    "if avg_cross_lang < 0.5:\n",
    "    print(f\"   1. ğŸš¨ DRINGEND: Multilingual Model verwenden!\")\n",
    "    print(f\"   2. ğŸ“‰ Similarity Threshold auf 0.3 reduzieren\")\n",
    "    print(f\"   3. ğŸ§ª Test: paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "else:\n",
    "    print(f\"   1. ğŸ“‰ Similarity Threshold anpassen\")\n",
    "    print(f\"   2. ğŸ”§ Query-Enhancement implementieren\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Based on the results above, our next notebook will test:**\n",
    "\n",
    "1. **Multilingual Models**: `paraphrase-multilingual-MiniLM-L12-v2`\n",
    "2. **Optimized Thresholds**: Find the sweet spot for your use case\n",
    "3. **Query Enhancement**: Add synonyms and translations\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ¯ **This notebook shows the EXACT problem** - no complex RAG pipeline needed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
