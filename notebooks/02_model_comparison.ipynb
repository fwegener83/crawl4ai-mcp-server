{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison for German/English Tech Docs\n",
    "\n",
    "**Ziel**: Verschiedene Embedding-Modelle direkt vergleichen\n",
    "\n",
    "**Test**: Current vs. Multilingual Models mit identischen Test-Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sentence_transformers import SentenceTransformer\nimport numpy as np\nimport time\nfrom typing import Dict, List, Tuple\n\ndef cosine_similarity(a, b):\n    \"\"\"Simple cosine similarity function\"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\nprint(\"‚úÖ Setup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data - Identical to previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deutsche Tech-Texte (typische Chunk-Inhalte)\n",
    "german_docs = [\n",
    "    \"Die Vektorsuche in ChromaDB verwendet Cosinus-√Ñhnlichkeit f√ºr semantische Suchen.\",\n",
    "    \"Chunking-Strategien sollten bei technischen Dokumentationen header-bewusst sein.\",\n",
    "    \"Der Similarity-Threshold von 0.7 ist oft zu restriktiv f√ºr multilinguale Inhalte.\",\n",
    "    \"RAG-Systeme ben√∂tigen optimierte Embeddings f√ºr bessere Retrieval-Performance.\"\n",
    "]\n",
    "\n",
    "# Englische Tech-Texte (semantisch verwandt)\n",
    "english_docs = [\n",
    "    \"Vector search in ChromaDB uses cosine similarity for semantic searches.\",\n",
    "    \"Chunking strategies should be header-aware for technical documentation.\",\n",
    "    \"The similarity threshold of 0.7 is often too restrictive for multilingual content.\",\n",
    "    \"RAG systems require optimized embeddings for better retrieval performance.\"\n",
    "]\n",
    "\n",
    "# Test-Queries (verschiedene Sprach-Kombinationen)\n",
    "test_queries = {\n",
    "    \"german_technical\": \"Wie optimiert man Vektorsuche?\",\n",
    "    \"english_technical\": \"How to optimize vector search?\",\n",
    "    \"mixed_query\": \"ChromaDB similarity threshold\",\n",
    "    \"german_rag\": \"RAG System Performance verbessern\",\n",
    "    \"english_rag\": \"improve RAG system performance\"\n",
    "}\n",
    "\n",
    "print(f\"üìù Test Data: {len(german_docs)} DE docs, {len(english_docs)} EN docs, {len(test_queries)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "**Modelle zum Vergleich:**\n",
    "1. **Current**: `all-MiniLM-L6-v2` (aktuell im RAG-System)\n",
    "2. **Multilingual Fast**: `paraphrase-multilingual-MiniLM-L12-v2` \n",
    "3. **Multilingual Quality**: `distiluse-base-multilingual-cased-v1`\n",
    "4. **Quality Baseline**: `all-mpnet-base-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "models_config = {\n",
    "    \"current\": {\n",
    "        \"name\": \"all-MiniLM-L6-v2\",\n",
    "        \"description\": \"Current RAG model (monolingual focus)\",\n",
    "        \"expected\": \"Poor cross-language performance\"\n",
    "    },\n",
    "    \"multilingual_fast\": {\n",
    "        \"name\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"description\": \"Multilingual, 50+ languages, fast\",\n",
    "        \"expected\": \"Good cross-language, balanced speed/quality\"\n",
    "    },\n",
    "    \"multilingual_quality\": {\n",
    "        \"name\": \"distiluse-base-multilingual-cased-v1\", \n",
    "        \"description\": \"Multilingual, 15 languages, quality focused\",\n",
    "        \"expected\": \"Best cross-language performance\"\n",
    "    },\n",
    "    \"quality_baseline\": {\n",
    "        \"name\": \"all-mpnet-base-v2\",\n",
    "        \"description\": \"High quality English (for comparison)\",\n",
    "        \"expected\": \"Best English quality, poor cross-language\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Models to test:\")\n",
    "for key, config in models_config.items():\n",
    "    print(f\"  {key}: {config['name']}\")\n",
    "    print(f\"    ‚Üí {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models (Progressive - to avoid memory issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load models progressively to manage memory\nmodels = {}\nmodel_stats = {}\n\ndef load_model(model_key: str, config: dict):\n    \"\"\"Load a single model and gather stats\"\"\"\n    import time  # Import time here to fix the error\n    \n    print(f\"\\nüîÑ Loading {model_key}: {config['name']}\")\n    \n    try:\n        start_time = time.time()\n        model = SentenceTransformer(config['name'])\n        load_time = time.time() - start_time\n        \n        # Basic model info\n        stats = {\n            \"name\": config['name'],\n            \"load_time\": load_time,\n            \"max_seq_length\": model.max_seq_length,\n            \"embedding_dimension\": model.get_sentence_embedding_dimension(),\n            \"description\": config['description']\n        }\n        \n        models[model_key] = model\n        model_stats[model_key] = stats\n        \n        print(f\"  ‚úÖ Loaded: {stats['embedding_dimension']}D, max_len={stats['max_seq_length']}, load_time={load_time:.2f}s\")\n        return True\n        \n    except Exception as e:\n        print(f\"  ‚ùå Failed to load: {e}\")\n        return False\n\n# Load each model (you can comment out models you don't want to test)\nprint(\"üì• Models will be automatically downloaded on first use...\")\nload_model(\"current\", models_config[\"current\"])\nload_model(\"multilingual_fast\", models_config[\"multilingual_fast\"])  \n# load_model(\"multilingual_quality\", models_config[\"multilingual_quality\"])\n# load_model(\"quality_baseline\", models_config[\"quality_baseline\"])\n\nprint(f\"\\nüìä Successfully loaded {len(models)} models\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Test\n",
    "\n",
    "**Test 1: Cross-Language Similarity Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cross_language_performance(model, model_name):\n",
    "    \"\"\"Test cross-language similarity for a single model\"\"\"\n",
    "    print(f\"\\nüîç Testing Cross-Language Performance: {model_name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Encode documents\n",
    "    start_time = time.time()\n",
    "    de_embeddings = model.encode(german_docs)\n",
    "    en_embeddings = model.encode(english_docs)\n",
    "    encoding_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate cross-language similarities (expected pairs)\n",
    "    similarities = []\n",
    "    for i in range(len(german_docs)):\n",
    "        sim = cosine_similarity(de_embeddings[i], en_embeddings[i])\n",
    "        similarities.append(sim)\n",
    "        \n",
    "        print(f\"  Pair {i+1}: {sim:.3f}\")\n",
    "        print(f\"    DE: {german_docs[i][:50]}...\")\n",
    "        print(f\"    EN: {english_docs[i][:50]}...\")\n",
    "    \n",
    "    avg_similarity = np.mean(similarities)\n",
    "    min_similarity = np.min(similarities)\n",
    "    max_similarity = np.max(similarities)\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  Average Cross-Lang Similarity: {avg_similarity:.3f}\")\n",
    "    print(f\"  Range: {min_similarity:.3f} - {max_similarity:.3f}\")\n",
    "    print(f\"  Encoding Time: {encoding_time:.3f}s for {len(german_docs + english_docs)} docs\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    if avg_similarity >= 0.7:\n",
    "        print(f\"  üéØ EXCELLENT cross-language performance!\")\n",
    "    elif avg_similarity >= 0.5:\n",
    "        print(f\"  ‚úÖ GOOD cross-language performance\")\n",
    "    elif avg_similarity >= 0.3:\n",
    "        print(f\"  ‚ö†Ô∏è  OKAY cross-language performance\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå POOR cross-language performance\")\n",
    "    \n",
    "    return {\n",
    "        \"avg_similarity\": avg_similarity,\n",
    "        \"min_similarity\": min_similarity,\n",
    "        \"max_similarity\": max_similarity,\n",
    "        \"encoding_time\": encoding_time,\n",
    "        \"individual_similarities\": similarities\n",
    "    }\n",
    "\n",
    "# Test all loaded models\n",
    "cross_lang_results = {}\n",
    "for model_key, model in models.items():\n",
    "    cross_lang_results[model_key] = test_cross_language_performance(model, model_stats[model_key]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Retrieval Comparison\n",
    "\n",
    "**Test 2: How do different models perform on actual queries?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_query_retrieval_comparison(models_dict, query_text, query_name):\n",
    "    \"\"\"Compare query performance across all models\"\"\"\n",
    "    print(f\"\\nüîç Query Comparison: {query_name}\")\n",
    "    print(f\"Query: '{query_text}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Combine all docs for retrieval\n",
    "    all_docs = german_docs + english_docs\n",
    "    \n",
    "    query_results = {}\n",
    "    \n",
    "    for model_key, model in models_dict.items():\n",
    "        model_name = model_stats[model_key]['name']\n",
    "        print(f\"\\nüì± Model: {model_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Encode query and docs\n",
    "        query_embedding = model.encode([query_text])[0]\n",
    "        doc_embeddings = model.encode(all_docs)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, (doc, doc_emb) in enumerate(zip(all_docs, doc_embeddings)):\n",
    "            sim = cosine_similarity(query_embedding, doc_emb)\n",
    "            lang = \"üá©üá™\" if i < len(german_docs) else \"üá¨üáß\"\n",
    "            similarities.append((sim, doc, lang, i+1))\n",
    "        \n",
    "        # Sort by relevance\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Show top 3 results\n",
    "        print(\"Top 3 Results:\")\n",
    "        for rank, (score, doc, lang, doc_id) in enumerate(similarities[:3], 1):\n",
    "            print(f\"  {rank}. [{score:.3f}] {lang} {doc[:50]}...\")\n",
    "        \n",
    "        # Threshold analysis\n",
    "        above_07 = sum(1 for s, _, _, _ in similarities if s >= 0.7)\n",
    "        above_05 = sum(1 for s, _, _, _ in similarities if s >= 0.5)\n",
    "        above_03 = sum(1 for s, _, _, _ in similarities if s >= 0.3)\n",
    "        \n",
    "        print(f\"  Threshold Analysis: 0.7‚Üí{above_07} | 0.5‚Üí{above_05} | 0.3‚Üí{above_03}\")\n",
    "        \n",
    "        query_results[model_key] = {\n",
    "            \"best_score\": similarities[0][0],\n",
    "            \"above_07\": above_07,\n",
    "            \"above_05\": above_05,\n",
    "            \"above_03\": above_03,\n",
    "            \"top_results\": similarities[:3]\n",
    "        }\n",
    "    \n",
    "    return query_results\n",
    "\n",
    "# Test all queries with all models\n",
    "query_comparison_results = {}\n",
    "for query_name, query_text in test_queries.items():\n",
    "    query_comparison_results[query_name] = test_query_retrieval_comparison(models, query_text, query_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comparison table (no pandas needed)\nprint(\"\\n\" + \"=\"*80)\nprint(f\"{'Model':<15} {'Name':<40} {'Dims':<6} {'CrossLang':<10} {'QueryAvg':<10} {'@0.7':<6} {'Time':<8}\")\nprint(\"-\"*80)\n\nfor data in comparison_data:\n    print(f\"{data['Model']:<15} {data['Name'][:40]:<40} {data['Dimensions']:<6} {data['Cross-Lang Avg']:<10} {data['Query Avg Score']:<10} {data['Results @ 0.7']:<6} {data['Encoding Time']:<8}\")\n\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Based on the results above:**\n",
    "\n",
    "1. **Best Model Identified** ‚úÖ\n",
    "2. **Optimal Threshold Found** ‚úÖ \n",
    "3. **Performance Expectations** ‚úÖ\n",
    "\n",
    "**Ready for Integration:**\n",
    "- Update RAG_MODEL_NAME in .env\n",
    "- Adjust similarity thresholds in code\n",
    "- Test with real RAG pipeline\n",
    "\n",
    "---\n",
    "üéØ **This gives you concrete data to make the switch!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}